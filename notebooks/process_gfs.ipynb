{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41001e6d-7153-4135-8940-41cfbcfe38b8",
   "metadata": {},
   "source": [
    "## Processing train gfs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262acfa1-10a4-46d7-b61c-6092aab8f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import pygrib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "WORK_DIR = Path('../data/')\n",
    "SAVE_DIR = Path('../data/raw/train_gfs_data/')\n",
    "!mkdir {SAVE_DIR}\n",
    "WORK_DIR.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5708e228-ad27-4e95-8ed3-a75d9f62cc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_data = pd.read_csv(WORK_DIR / 'grid_metadata.csv')\n",
    "grid_to_location = {g: l for g, l in zip(grid_data['grid_id'].values, grid_data['location'].values)}\n",
    "grid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a87829-f31d-4c86-859f-60194349f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(WORK_DIR / 'train_labels.csv')\n",
    "train_data['location'] = train_data['grid_id'].apply(lambda x: grid_to_location[x])\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc31aeb4-cc75-4d0b-885f-0b3a33f9f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[['datetime', 'location']]\n",
    "print(f\"Before dropping duplicates: {train_data.shape}\")\n",
    "train_data = train_data.drop_duplicates()\n",
    "print(f\"After dropping duplicates: {train_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39456b5e-9a38-45e2-8a35-f9d583a78caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_status(filepath, filesize):\n",
    "    # sys.stdout.write('\\r')\n",
    "    # sys.stdout.flush()\n",
    "    size = int(os.stat(filepath).st_size)\n",
    "    percent_complete = (size/filesize)*100\n",
    "    # sys.stdout.write('%.3f %s' % (percent_complete, '% Completed'))\n",
    "    # sys.stdout.flush()\n",
    "\n",
    "# Write your email and password of ncar account\n",
    "email = \"\"\n",
    "pswd = \"\"\n",
    "\n",
    "url = 'https://rda.ucar.edu/cgi-bin/login'\n",
    "values = {'email' : email, 'passwd' : pswd, 'action' : 'login'}\n",
    "# Authenticate\n",
    "ret = requests.post(url,data=values)\n",
    "if ret.status_code != 200:\n",
    "    print('Bad Authentication')\n",
    "    print(ret.text)\n",
    "    exit(1)\n",
    "DSPATH = 'https://rda.ucar.edu/data/ds084.1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b714b757-d8e4-401a-8288-7b56186421fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_map = {\n",
    "    'Los Angeles (SoCAB)': 'la',\n",
    "    'Taipei': 'tpe', \n",
    "    'Delhi': 'dl'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e6cbb7-c8b5-4b1b-a038-a72b3b3e9453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_cycle(dt):\n",
    "    hour = dt.hour\n",
    "    ans = None\n",
    "    for h in [0, 6, 12, 18]:\n",
    "        if hour >= h:\n",
    "            ans = h\n",
    "    return ans\n",
    "    \n",
    "def get_nearest_forecast(dt):\n",
    "    cycle = get_nearest_cycle(dt)\n",
    "    available_forecasts = [0, 3, 6, 9, 12, 15, 18, 21]\n",
    "    forecast = 0\n",
    "    for f in available_forecasts:\n",
    "        if cycle + f > dt.hour:\n",
    "            break\n",
    "        forecast = f\n",
    "    return \"{:02d}\".format(cycle), \"{:03d}\".format(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4b40c-0feb-43e0-90a6-526c2af8af1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [idx for idx in range(len(train_data))]\n",
    "variables = ['Wind speed (gust)', 'Haines Index', 'Surface pressure', 'Orography', 'Temperature', 'Water equivalent of accumulated snow depth (deprecated)', 'Snow depth', 'Potential evaporation rate', 'Percent frozen precipitation', 'Convective precipitation rate', 'Precipitation rate', 'Total Precipitation', 'Convective precipitation (water)', 'Water runoff', 'Categorical snow', 'Categorical ice pellets', 'Categorical freezing rain', 'Categorical rain', 'Latent heat net flux', 'Sensible heat net flux', 'Ground heat flux', 'Momentum flux, u component', 'Momentum flux, v component', 'Zonal flux of gravity wave stress', 'Meridional flux of gravity wave stress', 'Wilting Point', 'Field Capacity', 'Sunshine Duration', 'Surface lifted index', 'Convective available potential energy', 'Convective inhibition', 'Downward short-wave radiation flux', 'Downward long-wave radiation flux', 'Upward short-wave radiation flux', 'Upward long-wave radiation flux', 'Best (4-layer) lifted index', 'Planetary boundary layer height', 'Land-sea mask', 'Sea ice area fraction', 'Albedo', 'Land-sea coverage (nearest neighbor) [land=1,sea=0]']\n",
    "\n",
    "for idx in tqdm(indices):\n",
    "    el = train_data.iloc[idx]\n",
    "    dt = datetime.datetime.strptime(el['datetime'], '%Y-%m-%dT%H:%M:%SZ')\n",
    "    loc = location_map[el['location']]\n",
    "\n",
    "    cycle, forecast = get_nearest_forecast(dt)\n",
    "    year = dt.year\n",
    "    month = \"{:02d}\".format(dt.month)\n",
    "    day = \"{:02d}\".format(dt.day)\n",
    "    filename = f'{year}/{year}{month}{day}/gfs.0p25.{year}{month}{day}{cycle}.f{forecast}.grib2'\n",
    "    # print(f\"{dt} => {filename}\")\n",
    "\n",
    "    filename = DSPATH + filename\n",
    "    file_base = os.path.basename(filename)\n",
    "\n",
    "    req = requests.get(filename, cookies = ret.cookies, allow_redirects=True, stream=True)\n",
    "    filesize = int(req.headers['Content-length'])\n",
    "    with open(file_base, 'wb') as outfile:\n",
    "        chunk_size=1048576\n",
    "        for chunk in req.iter_content(chunk_size=chunk_size):\n",
    "            outfile.write(chunk)\n",
    "            if chunk_size < filesize:\n",
    "                check_file_status(file_base, filesize)\n",
    "    check_file_status(file_base, filesize)\n",
    "\n",
    "    gr = pygrib.open(file_base)\n",
    "    assets = {}\n",
    "\n",
    "    for g in gr:\n",
    "        if g.name in variables and g.typeOfLevel == 'surface':\n",
    "            assets[g.name] = g.values\n",
    "\n",
    "    assets['latitude'] = g.latlons()[0]\n",
    "    assets['longitude'] = g.latlons()[1]\n",
    "\n",
    "    np.savez_compressed(SAVE_DIR / f\"{el['datetime']}_{loc}\", **assets)\n",
    "    !rm {file_base}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5291f9bb-7f58-46e2-9683-a4d1e75f4b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from shapely.geometry import Point, Polygon\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "DATA_DIR = Path('../data/raw/train_gfs_data/')\n",
    "assert DATA_DIR.exists(), \"{} does not exist...\".format(DATA_DIR)\n",
    "REQUIRED_BANDS = [\n",
    "    'Wind speed (gust)', 'Haines Index', 'Surface pressure', \n",
    "    'Orography', 'Temperature', 'Water equivalent of accumulated snow depth (deprecated)', \n",
    "    'Snow depth', 'Percent frozen precipitation', 'Wilting Point', 'Field Capacity',\n",
    "    'Sunshine Duration', 'Surface lifted index', 'Convective available potential energy',\n",
    "    'Convective inhibition', 'Best (4-layer) lifted index', 'Planetary boundary layer height',\n",
    "    'Land-sea mask', 'Sea ice area fraction', 'Land-sea coverage (nearest neighbor) [land=1,sea=0]',\n",
    "    'latitude', 'longitude'\n",
    "]\n",
    "\n",
    "def floor(x, n=0):\n",
    "    return np.floor(x * 10**n) / 10**n\n",
    "\n",
    "def ceil(x, n=0):\n",
    "    return np.ceil(x * 10**n) / 10**n\n",
    "\n",
    "def get_bounds(geometry):\n",
    "    geometry = np.array(geometry)\n",
    "    long = [\n",
    "        floor(geometry[:, 0].min(), 1),\n",
    "        ceil(geometry[:, 0].max(), 1)\n",
    "    ]\n",
    "    lat = [\n",
    "        floor(geometry[:, 1].min(), 1),\n",
    "        ceil(geometry[:, 1].max(), 1)\n",
    "    ]\n",
    "    return long, lat\n",
    "\n",
    "def round_off(point, res):\n",
    "    spread = np.arange(np.floor(point), np.ceil(point) + 1, res)\n",
    "    adiff = np.abs(spread - point)\n",
    "    return spread[np.argmin(adiff)]\n",
    "\n",
    "def get_boundary(geometry, res):\n",
    "    long = np.array(geometry)[:, 0]\n",
    "    lat = np.array(geometry)[:, 1]\n",
    "    \n",
    "    min_lat, max_lat = lat.min(), lat.max()\n",
    "    min_long, max_long = long.min(), long.max()\n",
    "    \n",
    "    min_lat = round_off(min_lat - res / 2, res)\n",
    "    max_lat = round_off(max_lat + res / 2, res)\n",
    "    min_long = round_off(min_long - res / 2, res)\n",
    "    max_long = round_off(max_long + res / 2, res)\n",
    "    \n",
    "    return [min_long, max_long], [min_lat, max_lat]\n",
    "\n",
    "def mask(filename, geometry):\n",
    "    data = np.load(filename)\n",
    "    assets = {}\n",
    "    for key in data.keys():\n",
    "        assets[key] = data[key].ravel()\n",
    "    # longb, latb = get_bounds(geometry)\n",
    "    longb, latb = get_boundary(geometry, 0.25)\n",
    "    latitude = assets['latitude']\n",
    "    longitude = assets['longitude']\n",
    "    indices = (latitude >= latb[0]) & (latitude <= latb[1]) & (longitude >= longb[0]) & (longitude <= longb[1])\n",
    "\n",
    "    new_ass = {}\n",
    "    for k in assets.keys():\n",
    "        new_ass[k] = assets[k][indices]\n",
    "                                                                   \n",
    "    df = pd.DataFrame(new_ass)\n",
    "    return df\n",
    "\n",
    "train_data = pd.read_csv(\"../data/train_labels.csv\")\n",
    "grid_data = pd.read_csv(\"../data/grid_metadata.csv\")\n",
    "location_map = {\n",
    "    'Los Angeles (SoCAB)': 'la',\n",
    "    'Taipei': 'tpe', \n",
    "    'Delhi': 'dl'\n",
    "}\n",
    "\n",
    "indices = [idx for idx in range(len(train_data))]\n",
    "total_data = defaultdict(lambda: [])\n",
    "\n",
    "for idx in tqdm(indices):\n",
    "    el = train_data.iloc[idx]\n",
    "    grid_id = el['grid_id']\n",
    "    dt = el['datetime']\n",
    "\n",
    "    grid_el = grid_data.loc[grid_data['grid_id'] == grid_id]\n",
    "    loc = location_map[grid_el['location'].iloc[0]]\n",
    "    \n",
    "    geometry = grid_el['wkt'].values[0]\n",
    "    geometry = geometry.replace('(', '', -1)\n",
    "    geometry = geometry.replace(')', '', -1)\n",
    "    geometry = geometry.replace(',', '', -1)\n",
    "    geometry = list(map(float, geometry.split()[1:]))\n",
    "    geometry = [geometry[i:i+2] for i in range(0, len(geometry), 2)]\n",
    "\n",
    "    filename = '{}_{}.npz'.format(dt, loc)\n",
    "    filename = filename.replace(':', '_', -1)\n",
    "    filename = DATA_DIR / filename\n",
    "    try:\n",
    "        cur_data = mask(filename, geometry)\n",
    "    except Exception as e:\n",
    "        cur_data = pd.DataFrame({})\n",
    "        print(f\"{idx}: {e}\")\n",
    "\n",
    "    for key in REQUIRED_BANDS:\n",
    "        try:\n",
    "            _band = cur_data[key].values\n",
    "            _band = np.concatenate((\n",
    "                _band[_band <= 0], _band[_band > 0]\n",
    "            ))\n",
    "        except KeyError:\n",
    "            _band = np.array([np.nan, np.nan])\n",
    "        total_data[f\"{key}_mean\"].append(_band.mean())\n",
    "        total_data[f\"{key}_var\"].append(_band.std() ** 2)\n",
    "\n",
    "total_data = pd.DataFrame(total_data)\n",
    "total_data.to_csv('../data/proc/train_gfs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437cf88a-5844-4327-93bd-04fc78ebad26",
   "metadata": {},
   "source": [
    "## Processing test gfs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3081c7c-c1b3-49d1-8a08-04237ecc0fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import pygrib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "WORK_DIR = Path('../data/')\n",
    "SAVE_DIR = Path('../data/raw/test_gfs_data/')\n",
    "!mkdir {SAVE_DIR}\n",
    "WORK_DIR.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bcdcc1-6399-43c9-a348-7ad2fa8f5b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_data = pd.read_csv(WORK_DIR / 'grid_metadata.csv')\n",
    "grid_to_location = {g: l for g, l in zip(grid_data['grid_id'].values, grid_data['location'].values)}\n",
    "grid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc720cb-a2f0-4e1d-9a08-3ab84af32318",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(WORK_DIR / 'submission_format.csv')\n",
    "train_data['location'] = train_data['grid_id'].apply(lambda x: grid_to_location[x])\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ef50c2-ccfe-4821-b2b0-b17cd3b0632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[['datetime', 'location']]\n",
    "print(f\"Before dropping duplicates: {train_data.shape}\")\n",
    "train_data = train_data.drop_duplicates()\n",
    "print(f\"After dropping duplicates: {train_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6326017c-2907-41e8-8257-0eec01910ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_status(filepath, filesize):\n",
    "    # sys.stdout.write('\\r')\n",
    "    # sys.stdout.flush()\n",
    "    size = int(os.stat(filepath).st_size)\n",
    "    percent_complete = (size/filesize)*100\n",
    "    # sys.stdout.write('%.3f %s' % (percent_complete, '% Completed'))\n",
    "    # sys.stdout.flush()\n",
    "\n",
    "# Write your email and password of ncar account\n",
    "email = \"\"\n",
    "pswd = \"\"\n",
    "\n",
    "url = 'https://rda.ucar.edu/cgi-bin/login'\n",
    "values = {'email' : email, 'passwd' : pswd, 'action' : 'login'}\n",
    "# Authenticate\n",
    "ret = requests.post(url,data=values)\n",
    "if ret.status_code != 200:\n",
    "    print('Bad Authentication')\n",
    "    print(ret.text)\n",
    "    exit(1)\n",
    "DSPATH = 'https://rda.ucar.edu/data/ds084.1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e18c77-abb1-4389-bcd6-a786ae6057c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_map = {\n",
    "    'Los Angeles (SoCAB)': 'la',\n",
    "    'Taipei': 'tpe', \n",
    "    'Delhi': 'dl'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d4509e-dd09-4443-a0eb-4c0e55d32a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_cycle(dt):\n",
    "    hour = dt.hour\n",
    "    ans = None\n",
    "    for h in [0, 6, 12, 18]:\n",
    "        if hour >= h:\n",
    "            ans = h\n",
    "    return ans\n",
    "    \n",
    "def get_nearest_forecast(dt):\n",
    "    cycle = get_nearest_cycle(dt)\n",
    "    available_forecasts = [0, 3, 6, 9, 12, 15, 18, 21]\n",
    "    forecast = 0\n",
    "    for f in available_forecasts:\n",
    "        if cycle + f > dt.hour:\n",
    "            break\n",
    "        forecast = f\n",
    "    return \"{:02d}\".format(cycle), \"{:03d}\".format(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8c9504-9698-47e1-989c-a289182a1ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [idx for idx in range(len(train_data))]\n",
    "variables = ['Wind speed (gust)', 'Haines Index', 'Surface pressure', 'Orography', 'Temperature', 'Water equivalent of accumulated snow depth (deprecated)', 'Snow depth', 'Potential evaporation rate', 'Percent frozen precipitation', 'Convective precipitation rate', 'Precipitation rate', 'Total Precipitation', 'Convective precipitation (water)', 'Water runoff', 'Categorical snow', 'Categorical ice pellets', 'Categorical freezing rain', 'Categorical rain', 'Latent heat net flux', 'Sensible heat net flux', 'Ground heat flux', 'Momentum flux, u component', 'Momentum flux, v component', 'Zonal flux of gravity wave stress', 'Meridional flux of gravity wave stress', 'Wilting Point', 'Field Capacity', 'Sunshine Duration', 'Surface lifted index', 'Convective available potential energy', 'Convective inhibition', 'Downward short-wave radiation flux', 'Downward long-wave radiation flux', 'Upward short-wave radiation flux', 'Upward long-wave radiation flux', 'Best (4-layer) lifted index', 'Planetary boundary layer height', 'Land-sea mask', 'Sea ice area fraction', 'Albedo', 'Land-sea coverage (nearest neighbor) [land=1,sea=0]']\n",
    "\n",
    "for idx in tqdm(indices):\n",
    "    el = train_data.iloc[idx]\n",
    "    dt = datetime.datetime.strptime(el['datetime'], '%Y-%m-%dT%H:%M:%SZ')\n",
    "    loc = location_map[el['location']]\n",
    "\n",
    "    cycle, forecast = get_nearest_forecast(dt)\n",
    "    year = dt.year\n",
    "    month = \"{:02d}\".format(dt.month)\n",
    "    day = \"{:02d}\".format(dt.day)\n",
    "    filename = f'{year}/{year}{month}{day}/gfs.0p25.{year}{month}{day}{cycle}.f{forecast}.grib2'\n",
    "    # print(f\"{dt} => {filename}\")\n",
    "\n",
    "    filename = DSPATH + filename\n",
    "    file_base = os.path.basename(filename)\n",
    "\n",
    "    req = requests.get(filename, cookies = ret.cookies, allow_redirects=True, stream=True)\n",
    "    filesize = int(req.headers['Content-length'])\n",
    "    with open(file_base, 'wb') as outfile:\n",
    "        chunk_size=1048576\n",
    "        for chunk in req.iter_content(chunk_size=chunk_size):\n",
    "            outfile.write(chunk)\n",
    "            if chunk_size < filesize:\n",
    "                check_file_status(file_base, filesize)\n",
    "    check_file_status(file_base, filesize)\n",
    "\n",
    "    gr = pygrib.open(file_base)\n",
    "    assets = {}\n",
    "\n",
    "    for g in gr:\n",
    "        if g.name in variables and g.typeOfLevel == 'surface':\n",
    "            assets[g.name] = g.values\n",
    "\n",
    "    assets['latitude'] = g.latlons()[0]\n",
    "    assets['longitude'] = g.latlons()[1]\n",
    "\n",
    "    np.savez_compressed(SAVE_DIR / f\"{el['datetime']}_{loc}\", **assets)\n",
    "    !rm {file_base}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73c5207-8dcb-40b8-a9f1-ab47777b4b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from shapely.geometry import Point, Polygon\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "DATA_DIR = Path('../data/raw/test_gfs_data/')\n",
    "assert DATA_DIR.exists(), \"{} does not exist...\".format(DATA_DIR)\n",
    "REQUIRED_BANDS = [\n",
    "    'Wind speed (gust)', 'Haines Index', 'Surface pressure', \n",
    "    'Orography', 'Temperature', 'Water equivalent of accumulated snow depth (deprecated)', \n",
    "    'Snow depth', 'Percent frozen precipitation', 'Wilting Point', 'Field Capacity',\n",
    "    'Sunshine Duration', 'Surface lifted index', 'Convective available potential energy',\n",
    "    'Convective inhibition', 'Best (4-layer) lifted index', 'Planetary boundary layer height',\n",
    "    'Land-sea mask', 'Sea ice area fraction', 'Land-sea coverage (nearest neighbor) [land=1,sea=0]',\n",
    "    'latitude', 'longitude'\n",
    "]\n",
    "\n",
    "def floor(x, n=0):\n",
    "    return np.floor(x * 10**n) / 10**n\n",
    "\n",
    "def ceil(x, n=0):\n",
    "    return np.ceil(x * 10**n) / 10**n\n",
    "\n",
    "def get_bounds(geometry):\n",
    "    geometry = np.array(geometry)\n",
    "    long = [\n",
    "        floor(geometry[:, 0].min(), 1),\n",
    "        ceil(geometry[:, 0].max(), 1)\n",
    "    ]\n",
    "    lat = [\n",
    "        floor(geometry[:, 1].min(), 1),\n",
    "        ceil(geometry[:, 1].max(), 1)\n",
    "    ]\n",
    "    return long, lat\n",
    "\n",
    "def round_off(point, res):\n",
    "    spread = np.arange(np.floor(point), np.ceil(point) + 1, res)\n",
    "    adiff = np.abs(spread - point)\n",
    "    return spread[np.argmin(adiff)]\n",
    "\n",
    "def get_boundary(geometry, res):\n",
    "    long = np.array(geometry)[:, 0]\n",
    "    lat = np.array(geometry)[:, 1]\n",
    "    \n",
    "    min_lat, max_lat = lat.min(), lat.max()\n",
    "    min_long, max_long = long.min(), long.max()\n",
    "    \n",
    "    min_lat = round_off(min_lat - res / 2, res)\n",
    "    max_lat = round_off(max_lat + res / 2, res)\n",
    "    min_long = round_off(min_long - res / 2, res)\n",
    "    max_long = round_off(max_long + res / 2, res)\n",
    "    \n",
    "    return [min_long, max_long], [min_lat, max_lat]\n",
    "\n",
    "def mask(filename, geometry):\n",
    "    data = np.load(filename)\n",
    "    assets = {}\n",
    "    for key in data.keys():\n",
    "        assets[key] = data[key].ravel()\n",
    "    # longb, latb = get_bounds(geometry)\n",
    "    longb, latb = get_boundary(geometry, 0.25)\n",
    "    latitude = assets['latitude']\n",
    "    longitude = assets['longitude']\n",
    "    indices = (latitude >= latb[0]) & (latitude <= latb[1]) & (longitude >= longb[0]) & (longitude <= longb[1])\n",
    "\n",
    "    new_ass = {}\n",
    "    for k in assets.keys():\n",
    "        new_ass[k] = assets[k][indices]\n",
    "                                                                   \n",
    "    df = pd.DataFrame(new_ass)\n",
    "    return df\n",
    "\n",
    "train_data = pd.read_csv(\"../data/submission_format.csv\")\n",
    "grid_data = pd.read_csv(\"../data/grid_metadata.csv\")\n",
    "location_map = {\n",
    "    'Los Angeles (SoCAB)': 'la',\n",
    "    'Taipei': 'tpe', \n",
    "    'Delhi': 'dl'\n",
    "}\n",
    "\n",
    "indices = [idx for idx in range(len(train_data))]\n",
    "total_data = defaultdict(lambda: [])\n",
    "\n",
    "for idx in tqdm(indices):\n",
    "    el = train_data.iloc[idx]\n",
    "    grid_id = el['grid_id']\n",
    "    dt = el['datetime']\n",
    "\n",
    "    grid_el = grid_data.loc[grid_data['grid_id'] == grid_id]\n",
    "    loc = location_map[grid_el['location'].iloc[0]]\n",
    "    \n",
    "    geometry = grid_el['wkt'].values[0]\n",
    "    geometry = geometry.replace('(', '', -1)\n",
    "    geometry = geometry.replace(')', '', -1)\n",
    "    geometry = geometry.replace(',', '', -1)\n",
    "    geometry = list(map(float, geometry.split()[1:]))\n",
    "    geometry = [geometry[i:i+2] for i in range(0, len(geometry), 2)]\n",
    "\n",
    "    filename = '{}_{}.npz'.format(dt, loc)\n",
    "    filename = filename.replace(':', '_', -1)\n",
    "    filename = DATA_DIR / filename\n",
    "    try:\n",
    "        cur_data = mask(filename, geometry)\n",
    "    except Exception as e:\n",
    "        cur_data = pd.DataFrame({})\n",
    "        print(f\"{idx}: {e}\")\n",
    "\n",
    "    for key in REQUIRED_BANDS:\n",
    "        try:\n",
    "            _band = cur_data[key].values\n",
    "            _band = np.concatenate((\n",
    "                _band[_band <= 0], _band[_band > 0]\n",
    "            ))\n",
    "        except KeyError:\n",
    "            _band = np.array([np.nan, np.nan])\n",
    "        total_data[f\"{key}_mean\"].append(_band.mean())\n",
    "        total_data[f\"{key}_var\"].append(_band.std() ** 2)\n",
    "\n",
    "total_data = pd.DataFrame(total_data)\n",
    "total_data.to_csv('../data/proc/test_gfs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
