{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c7533fa-572c-4569-ac26-0a2dcc580f06",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Compressing MISR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1572edf0-8f04-4e2c-8e0b-ec6ef23e6dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import rasterio as rio\n",
    "import rioxarray as rxr\n",
    "import gc;gc.enable()\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "DATA_DIR = '../data/raw/data'\n",
    "PRODUCT = 'misr'\n",
    "LOCATION_MAP = {'Taipei': 'tpe', 'Delhi': 'dl', 'Los Angeles (SoCAB)': 'la'}\n",
    "\n",
    "satellite_data = pd.read_csv('pm25_satellite_metadata.csv')\n",
    "satellite_data = satellite_data[satellite_data['product'] == PRODUCT]\n",
    "satellite_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e40811a-72a4-4fd6-91c5-844b831e6b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIRED_BANDS = ['Latitude', 'Longitude', 'Aerosol_Optical_Depth', \n",
    "                  'Aerosol_Optical_Depth_Uncertainty', 'Angstrom_Exponent_550_860nm', \n",
    "                  'Spectral_AOD_Scaling_Coeff', 'Absorption_Aerosol_Optical_Depth',\n",
    "                  'Nonspherical_Aerosol_Optical_Depth', 'Small_Mode_Aerosol_Optical_Depth',\n",
    "                  'Medium_Mode_Aerosol_Optical_Depth', 'Large_Mode_Aerosol_Optical_Depth']\n",
    "def fillna(data, col):\n",
    "    temp = data[col]\n",
    "    fillvalue = temp._FillValue\n",
    "    temp = np.array(temp)\n",
    "    temp[temp == fillvalue] = np.nan\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e2da4-3f36-4a0c-9f22-3a3620c8ca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = '../data/raw/proc_misr'\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)\n",
    "\n",
    "def load_and_save(filename, name):\n",
    "    data = netCDF4.Dataset(filename, mode='r')\n",
    "    data = data.groups['4.4_KM_PRODUCTS'].variables\n",
    "    assets = {}\n",
    "    for band in REQUIRED_BANDS:\n",
    "        assets[band] = fillna(data, band)\n",
    "    np.savez_compressed(os.path.join(SAVE_DIR, f\"{name[:-3]}.npz\"), **assets)\n",
    "\n",
    "for idx in tqdm(range(len(satellite_data))):\n",
    "    el = satellite_data.iloc[idx]\n",
    "    name = el['granule_id']\n",
    "    filename = os.path.join(DATA_DIR, el['split'], name[:4], name)\n",
    "    load_and_save(filename, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345a8273-c7ae-4eac-bf9a-7216229bdba4",
   "metadata": {},
   "source": [
    "## Processing train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3173386-f5ae-4df2-baa5-7984b443eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import gc;gc.enable()\n",
    "from collections import defaultdict\n",
    "from shapely.geometry import Point, Polygon, box\n",
    "from tqdm import tqdm\n",
    "warnings.simplefilter('ignore')\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('../data/raw/proc_misr')\n",
    "PRODUCT = 'misr'\n",
    "SPLIT = 'train'\n",
    "LOCATION_MAP = {'Taipei': 'tpe', 'Delhi': 'dl', 'Los Angeles (SoCAB)': 'la'}\n",
    "\n",
    "train_data = pd.read_csv('../data/train_labels.csv')\n",
    "satellite_data = pd.read_csv('../data/pm25_satellite_metadata.csv')\n",
    "satellite_data = satellite_data[satellite_data['split'] == SPLIT]\n",
    "satellite_data = satellite_data[satellite_data['product'] == PRODUCT]\n",
    "satellite_data['time_end'] = satellite_data['time_end'].apply(\n",
    "    lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S+00:00')\n",
    ")\n",
    "grid_data = pd.read_csv('../data/grid_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beb03c2-be33-4677-87e6-b98b6dc6ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "total_data = defaultdict(lambda: [])\n",
    "REQUIRED_BANDS = ['Latitude', 'Longitude', 'Aerosol_Optical_Depth', \n",
    "                  'Aerosol_Optical_Depth_Uncertainty', 'Angstrom_Exponent_550_860nm', \n",
    "                  'Absorption_Aerosol_Optical_Depth',\n",
    "                  'Nonspherical_Aerosol_Optical_Depth', 'Small_Mode_Aerosol_Optical_Depth',\n",
    "                  'Medium_Mode_Aerosol_Optical_Depth', 'Large_Mode_Aerosol_Optical_Depth']\n",
    "LOCATION_MAP = {\n",
    "    'Taipei': 'tpe',\n",
    "    'Delhi': 'dl',\n",
    "    'Los Angeles (SoCAB)': 'la'\n",
    "}\n",
    "LOC_GRIDS = {}\n",
    "for location in LOCATION_MAP.keys():\n",
    "    LOC_GRIDS[\n",
    "        LOCATION_MAP[location]\n",
    "    ] = grid_data[grid_data['location'] == location]['grid_id'].values.tolist()\n",
    "\n",
    "GRID_GEOMETRY = {}\n",
    "for grid in grid_data['grid_id'].unique():\n",
    "    geometry = grid_data[grid_data['grid_id'] == grid]['wkt'].values[0]\n",
    "    geometry = geometry.replace('(', '', -1)\n",
    "    geometry = geometry.replace(')', '', -1)\n",
    "    geometry = geometry.replace(',', '', -1)\n",
    "    geometry = list(map(float, geometry.split()[1:]))\n",
    "    geometry = [geometry[i:i+2] for i in range(0, len(geometry), 2)]\n",
    "\n",
    "    GRID_GEOMETRY[grid] = geometry\n",
    "DATA_DIR = '../data/raw/proc_misr'\n",
    "\n",
    "def box_mask(filename, geometries):\n",
    "    data = np.load(filename)\n",
    "    assets = {}\n",
    "    for key in data.keys():\n",
    "        assets[key] = data[key].ravel()\n",
    "        shape = len(assets[key])\n",
    "    \n",
    "    indices = np.array([False for _ in range(shape)])\n",
    "    for geometry in geometries:\n",
    "        longb, latb = get_bounds(geometry)\n",
    "        latitude = assets['Latitude']\n",
    "        longitude = assets['Longitude']\n",
    "        cur_indices = (latitude >= latb[0]) & (latitude <= latb[1]) & (longitude >= longb[0]) & (longitude <= longb[1])\n",
    "        indices = indices | cur_indices\n",
    "    \n",
    "    new_ass = {}\n",
    "    for k in REQUIRED_BANDS:\n",
    "        new_ass[k] = assets[k][indices]\n",
    "    # new_ass['geometry'] = [Point(lg, lt) for lg, lt in zip(new_ass['longitude'], new_ass['latitude'])]\n",
    "    return new_ass\n",
    "\n",
    "def poly_mask(assets, geometry):                                                                   \n",
    "    df = pd.DataFrame(assets)\n",
    "\n",
    "    longb, latb = get_bounds(geometry)\n",
    "    latitude = assets['Latitude']\n",
    "    longitude = assets['Longitude']\n",
    "    cur_indices = (latitude >= latb[0]) & (latitude <= latb[1]) & (longitude >= longb[0]) & (longitude <= longb[1])\n",
    "    df = df.loc[cur_indices]\n",
    "    return df\n",
    "\n",
    "def get_data(filename, geometries, grid_ids, data_dict):\n",
    "    assets = box_mask(filename, geometries)\n",
    "    for geometry, grid_id in zip(geometries, grid_ids):\n",
    "        new_ass = poly_mask(assets, geometry)\n",
    "        data_dict['file_id'].append(\n",
    "            os.path.split(filename)[-1].replace('.npz', '') + f\"_{grid_id}\"\n",
    "        )\n",
    "        for key in REQUIRED_BANDS:\n",
    "            _band = new_ass[key].values\n",
    "            _band = np.concatenate((\n",
    "                _band[_band <= 0], _band[_band > 0]\n",
    "            ))\n",
    "            data_dict[f\"{key}_mean\"].append(_band.mean())\n",
    "            data_dict[f\"{key}_var\"].append(_band.std() ** 2)\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "for idx in tqdm(range(satellite_data.shape[0])):\n",
    "    filename = satellite_data['granule_id'].values[idx]\n",
    "    filename = filename.replace('.nc', '.npz')\n",
    "    filename = os.path.join(DATA_DIR, filename)\n",
    "    location = satellite_data['location'].values[idx]\n",
    "\n",
    "    geometries = [GRID_GEOMETRY[grid_id] for grid_id in LOC_GRIDS[location]]\n",
    "    total_data = get_data(filename, geometries, LOC_GRIDS[location], total_data)\n",
    "    \n",
    "total_data = pd.DataFrame(total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151e3f0c-4ab8-4f42-9b80-2dec245ba6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIRED_BANDS = ['Latitude', 'Longitude', 'Aerosol_Optical_Depth', \n",
    "                  'Aerosol_Optical_Depth_Uncertainty', 'Angstrom_Exponent_550_860nm', \n",
    "                  'Absorption_Aerosol_Optical_Depth',\n",
    "                  'Nonspherical_Aerosol_Optical_Depth', 'Small_Mode_Aerosol_Optical_Depth',\n",
    "                  'Medium_Mode_Aerosol_Optical_Depth', 'Large_Mode_Aerosol_Optical_Depth']\n",
    "indices = list(range(len(train_data)))\n",
    "total_train_data = defaultdict(lambda: [])\n",
    "\n",
    "for idx in tqdm(indices):\n",
    "    el = train_data.iloc[idx]\n",
    "    grid_id = el['grid_id']\n",
    "    location = grid_data[grid_data['grid_id'] == grid_id]['location'].values[0]\n",
    "    cur_satdata = satellite_data[satellite_data['location'] == LOCATION_MAP[location]]\n",
    "\n",
    "    dt = datetime.datetime.strptime(el['datetime'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    dt = dt + datetime.timedelta(1, 0)\n",
    "    possible = cur_satdata[cur_satdata['time_end'] < dt].sort_values('time_end', ascending=False).reset_index()\n",
    "    \n",
    "    if len(possible) == 0:\n",
    "        for band in REQUIRED_BANDS:\n",
    "            total_train_data[f\"{band}_mean\"].append(np.nan)\n",
    "            total_train_data[f\"{band}_var\"].append(np.nan)\n",
    "        total_train_data['filename'].append(np.nan)\n",
    "        continue\n",
    "    \n",
    "    filename = possible['granule_id'].iloc[0]\n",
    "    filename = f\"{filename[:-3]}_{grid_id}\"\n",
    "    \n",
    "    # cur_data = mask(filename, geometry)\n",
    "    cur_data = total_data[total_data['file_id'] == filename]\n",
    "    for key in REQUIRED_BANDS:\n",
    "        total_train_data[f\"{key}_mean\"].append(\n",
    "            cur_data[f\"{key}_mean\"].values[0]\n",
    "        )\n",
    "        total_train_data[f\"{key}_var\"].append(\n",
    "            cur_data[f\"{key}_var\"].values[0]\n",
    "        ) \n",
    "    total_train_data['filename'].append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a3bb7f-9e98-48c9-9bd1-0d156896d7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_data = pd.DataFrame(total_train_data)\n",
    "total_train_data.to_csv('../data/proc/train_misr.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e9d928-b6be-4317-9f48-da865590eec4",
   "metadata": {},
   "source": [
    "## Processing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6ba95d-7d86-4444-9e39-2f86f27d7acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import gc;gc.enable()\n",
    "from collections import defaultdict\n",
    "from shapely.geometry import Point, Polygon, box\n",
    "from tqdm import tqdm\n",
    "warnings.simplefilter('ignore')\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('../data/raw/proc_misr')\n",
    "PRODUCT = 'misr'\n",
    "SPLIT = 'test'\n",
    "LOCATION_MAP = {'Taipei': 'tpe', 'Delhi': 'dl', 'Los Angeles (SoCAB)': 'la'}\n",
    "\n",
    "train_data = pd.read_csv('../data/submission_format.csv')\n",
    "satellite_data = pd.read_csv('../data/pm25_satellite_metadata.csv')\n",
    "satellite_data = satellite_data[satellite_data['split'] == SPLIT]\n",
    "satellite_data = satellite_data[satellite_data['product'] == PRODUCT]\n",
    "satellite_data['time_end'] = satellite_data['time_end'].apply(\n",
    "    lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S+00:00')\n",
    ")\n",
    "grid_data = pd.read_csv('../data/grid_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e3f360-8358-44dd-80d0-121bd0cd2009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "total_data = defaultdict(lambda: [])\n",
    "REQUIRED_BANDS = ['Latitude', 'Longitude', 'Aerosol_Optical_Depth', \n",
    "                  'Aerosol_Optical_Depth_Uncertainty', 'Angstrom_Exponent_550_860nm', \n",
    "                  'Absorption_Aerosol_Optical_Depth',\n",
    "                  'Nonspherical_Aerosol_Optical_Depth', 'Small_Mode_Aerosol_Optical_Depth',\n",
    "                  'Medium_Mode_Aerosol_Optical_Depth', 'Large_Mode_Aerosol_Optical_Depth']\n",
    "LOCATION_MAP = {\n",
    "    'Taipei': 'tpe',\n",
    "    'Delhi': 'dl',\n",
    "    'Los Angeles (SoCAB)': 'la'\n",
    "}\n",
    "LOC_GRIDS = {}\n",
    "for location in LOCATION_MAP.keys():\n",
    "    LOC_GRIDS[\n",
    "        LOCATION_MAP[location]\n",
    "    ] = grid_data[grid_data['location'] == location]['grid_id'].values.tolist()\n",
    "\n",
    "GRID_GEOMETRY = {}\n",
    "for grid in grid_data['grid_id'].unique():\n",
    "    geometry = grid_data[grid_data['grid_id'] == grid]['wkt'].values[0]\n",
    "    geometry = geometry.replace('(', '', -1)\n",
    "    geometry = geometry.replace(')', '', -1)\n",
    "    geometry = geometry.replace(',', '', -1)\n",
    "    geometry = list(map(float, geometry.split()[1:]))\n",
    "    geometry = [geometry[i:i+2] for i in range(0, len(geometry), 2)]\n",
    "\n",
    "    GRID_GEOMETRY[grid] = geometry\n",
    "DATA_DIR = 'proc_misr'\n",
    "\n",
    "def box_mask(filename, geometries):\n",
    "    data = np.load(filename)\n",
    "    assets = {}\n",
    "    for key in data.keys():\n",
    "        assets[key] = data[key].ravel()\n",
    "        shape = len(assets[key])\n",
    "    \n",
    "    indices = np.array([False for _ in range(shape)])\n",
    "    for geometry in geometries:\n",
    "        longb, latb = get_bounds(geometry)\n",
    "        latitude = assets['Latitude']\n",
    "        longitude = assets['Longitude']\n",
    "        cur_indices = (latitude >= latb[0]) & (latitude <= latb[1]) & (longitude >= longb[0]) & (longitude <= longb[1])\n",
    "        indices = indices | cur_indices\n",
    "    \n",
    "    new_ass = {}\n",
    "    for k in REQUIRED_BANDS:\n",
    "        new_ass[k] = assets[k][indices]\n",
    "    # new_ass['geometry'] = [Point(lg, lt) for lg, lt in zip(new_ass['longitude'], new_ass['latitude'])]\n",
    "    return new_ass\n",
    "\n",
    "def poly_mask(assets, geometry):                                                                   \n",
    "    df = pd.DataFrame(assets)\n",
    "\n",
    "    longb, latb = get_bounds(geometry)\n",
    "    latitude = assets['Latitude']\n",
    "    longitude = assets['Longitude']\n",
    "    cur_indices = (latitude >= latb[0]) & (latitude <= latb[1]) & (longitude >= longb[0]) & (longitude <= longb[1])\n",
    "    df = df.loc[cur_indices]\n",
    "    return df\n",
    "\n",
    "def get_data(filename, geometries, grid_ids, data_dict):\n",
    "    assets = box_mask(filename, geometries)\n",
    "    for geometry, grid_id in zip(geometries, grid_ids):\n",
    "        new_ass = poly_mask(assets, geometry)\n",
    "        data_dict['file_id'].append(\n",
    "            os.path.split(filename)[-1].replace('.npz', '') + f\"_{grid_id}\"\n",
    "        )\n",
    "        for key in REQUIRED_BANDS:\n",
    "            _band = new_ass[key].values\n",
    "            _band = np.concatenate((\n",
    "                _band[_band <= 0], _band[_band > 0]\n",
    "            ))\n",
    "            data_dict[f\"{key}_mean\"].append(_band.mean())\n",
    "            data_dict[f\"{key}_var\"].append(_band.std() ** 2)\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "for idx in tqdm(range(satellite_data.shape[0])):\n",
    "    filename = satellite_data['granule_id'].values[idx]\n",
    "    filename = filename.replace('.nc', '.npz')\n",
    "    filename = os.path.join(DATA_DIR, filename)\n",
    "    location = satellite_data['location'].values[idx]\n",
    "\n",
    "    geometries = [GRID_GEOMETRY[grid_id] for grid_id in LOC_GRIDS[location]]\n",
    "    total_data = get_data(filename, geometries, LOC_GRIDS[location], total_data)\n",
    "    \n",
    "total_data = pd.DataFrame(total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1565aa7-e631-40e1-87fe-cc0c6661f1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIRED_BANDS = ['Latitude', 'Longitude', 'Aerosol_Optical_Depth', \n",
    "                  'Aerosol_Optical_Depth_Uncertainty', 'Angstrom_Exponent_550_860nm', \n",
    "                  'Absorption_Aerosol_Optical_Depth',\n",
    "                  'Nonspherical_Aerosol_Optical_Depth', 'Small_Mode_Aerosol_Optical_Depth',\n",
    "                  'Medium_Mode_Aerosol_Optical_Depth', 'Large_Mode_Aerosol_Optical_Depth']\n",
    "indices = list(range(len(train_data)))\n",
    "total_train_data = defaultdict(lambda: [])\n",
    "\n",
    "for idx in tqdm(indices):\n",
    "    el = train_data.iloc[idx]\n",
    "    grid_id = el['grid_id']\n",
    "    location = grid_data[grid_data['grid_id'] == grid_id]['location'].values[0]\n",
    "    cur_satdata = satellite_data[satellite_data['location'] == LOCATION_MAP[location]]\n",
    "\n",
    "    dt = datetime.datetime.strptime(el['datetime'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    dt = dt + datetime.timedelta(1, 0)\n",
    "    possible = cur_satdata[cur_satdata['time_end'] < dt].sort_values('time_end', ascending=False).reset_index()\n",
    "    \n",
    "    if len(possible) == 0:\n",
    "        for band in REQUIRED_BANDS:\n",
    "            total_train_data[f\"{band}_mean\"].append(np.nan)\n",
    "            total_train_data[f\"{band}_var\"].append(np.nan)\n",
    "        total_train_data['filename'].append(np.nan)\n",
    "        continue\n",
    "    \n",
    "    filename = possible['granule_id'].iloc[0]\n",
    "    filename = f\"{filename[:-3]}_{grid_id}\"\n",
    "    \n",
    "    # cur_data = mask(filename, geometry)\n",
    "    cur_data = total_data[total_data['file_id'] == filename]\n",
    "    for key in REQUIRED_BANDS:\n",
    "        total_train_data[f\"{key}_mean\"].append(\n",
    "            cur_data[f\"{key}_mean\"].values[0]\n",
    "        )\n",
    "        total_train_data[f\"{key}_var\"].append(\n",
    "            cur_data[f\"{key}_var\"].values[0]\n",
    "        ) \n",
    "    total_train_data['filename'].append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcc2f19-0485-45e4-bd8e-24df31977da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_data = pd.DataFrame(total_train_data)\n",
    "total_train_data.to_csv('../data/proc/test_misr.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
